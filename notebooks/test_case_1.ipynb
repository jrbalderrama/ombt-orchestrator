{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-mortem analysis\n",
    "\n",
    "Direct links : \n",
    "\n",
    "* [Ombt-statistics](#Ombt-statistics)\n",
    "* [Influxdb-Metrics](#Influxdb-Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ombt statistics\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the env dir of the experimental campaign\n",
    "RESULT_PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting some ombt code (this could be removed when used as a library)\n",
    "# This is used to recover the global stats from the per-agent stats\n",
    "# Per agent stats are outputed from the controller in a dedicated.\n",
    "import math\n",
    "\n",
    "class Stats(object):\n",
    "    \"\"\"Manage a single statistic\"\"\"\n",
    "    def __init__(self, min=None, max=None, total=0, count=0,\n",
    "                 sum_of_squares=0, distribution=None):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.total = total\n",
    "        self.count = count\n",
    "        self.sum_of_squares = sum_of_squares\n",
    "        # distribution of values grouped by powers of 10\n",
    "        self.distribution = distribution or dict()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, values):\n",
    "        if 'distribution' in values:\n",
    "            # hack alert!\n",
    "            # when a Stats is passed via an RPC call it appears as if the\n",
    "            # distribution map's keys are converted from int to str.\n",
    "            # Fix that by re-indexing the distribution map:\n",
    "            new_dict = dict()\n",
    "            old_dict = values['distribution']\n",
    "            for k in old_dict.keys():\n",
    "                new_dict[int(k)] = old_dict[k];\n",
    "            values['distribution'] = new_dict\n",
    "        return Stats(**values)\n",
    "\n",
    "    def to_dict(self):\n",
    "        new_dict = dict()\n",
    "        for a in [\"min\", \"max\", \"total\", \"count\", \"sum_of_squares\"]:\n",
    "            new_dict[a] = getattr(self, a)\n",
    "        new_dict[\"distribution\"] = self.distribution.copy()\n",
    "        return new_dict\n",
    "\n",
    "    def update(self, value):\n",
    "        self.total += value\n",
    "        self.count += 1\n",
    "        self.sum_of_squares += value**2\n",
    "        self.min = min(self.min, value) if self.min else value\n",
    "        self.max = max(self.max, value) if self.max else value\n",
    "        log = int(math.log10(value)) if value >= 1.0 else 0\n",
    "        base = 10**log\n",
    "        index = int(value/base)  # 0..9\n",
    "        if log not in self.distribution:\n",
    "            self.distribution[log] = [0 for i in range(10)]\n",
    "        self.distribution[log][index] += 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def average(self):\n",
    "        return (self.total / float(self.count)) if self.count else 0\n",
    "\n",
    "    def std_deviation(self):\n",
    "        return math.sqrt((self.sum_of_squares / float(self.count))\n",
    "                         - (self.average() ** 2)) if self.count else -1\n",
    "\n",
    "    def merge(self, stats):\n",
    "        if stats.min is not None and self.min is not None:\n",
    "            self.min = min(self.min, stats.min)\n",
    "        else:\n",
    "            self.min = self.min or stats.min\n",
    "        if stats.max is not None and self.max is not None:\n",
    "            self.max = max(self.max, stats.max)\n",
    "        else:\n",
    "            self.max = self.max or stats.max\n",
    "\n",
    "        self.total += stats.total\n",
    "        self.count += stats.count\n",
    "        self.sum_of_squares += stats.sum_of_squares\n",
    "        for k in stats.distribution.keys():\n",
    "            if k in self.distribution:\n",
    "                self.distribution[k] = [z for z in map(lambda a, b: a + b,\n",
    "                                                       stats.distribution[k],\n",
    "                                                       self.distribution[k])]\n",
    "            else:\n",
    "                self.distribution[k] = stats.distribution[k]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"min=%i, max=%i, avg=%f, std-dev=%f\" % (self.min, self.max,\n",
    "                                                       self.average(),\n",
    "                                                       self.std_deviation())\n",
    "\n",
    "    def print_distribution(self):\n",
    "        keys = list(self.distribution.keys())\n",
    "        keys.sort()\n",
    "        for order in keys:\n",
    "            row = self.distribution[order]\n",
    "            # order=0, index=0 is special case as it is < 1.0, for all orders >\n",
    "            # 0, index 0 is ignored since everthing < 10^order is accounted for\n",
    "            # in index 9 of the (order - 1) row\n",
    "            index = 0 if order == 0 else 1\n",
    "            while index < len(row):\n",
    "                print(\"[%d..<%d):  %d\" %\n",
    "                      ((10 ** int(order)) * index,\n",
    "                       (10 ** int(order)) * (index + 1),\n",
    "                       row[index]))\n",
    "                index += 1\n",
    "\n",
    "class TestResults(object):\n",
    "    \"\"\"Client results of a test run.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_time=None, stop_time=None, latency=None,\n",
    "                 msgs_ok=0, msgs_fail=0, errors=None):\n",
    "        super(TestResults, self).__init__()\n",
    "        self.start_time = start_time\n",
    "        self.stop_time = stop_time\n",
    "        self.latency = latency or Stats()\n",
    "        self.msgs_ok = msgs_ok  # count of successful msg transfers\n",
    "        self.msgs_fail = msgs_fail  # count of failed msg transfers\n",
    "        self.errors = errors or dict()  # error msgs and counts\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, values):\n",
    "        if 'latency' in values:\n",
    "            values['latency'] = Stats.from_dict(values['latency'])\n",
    "        if 'errors' in values:\n",
    "            values['errors'] = values['errors'].copy()\n",
    "        return TestResults(**values)\n",
    "\n",
    "    def to_dict(self):\n",
    "        new_dict = dict()\n",
    "        for a in ['start_time', 'stop_time', 'msgs_ok', 'msgs_fail']:\n",
    "            new_dict[a] = getattr(self, a)\n",
    "        new_dict['latency'] = self.latency.to_dict()\n",
    "        new_dict['errors'] = self.errors.copy()\n",
    "        return new_dict\n",
    "\n",
    "    def error(self, reason):\n",
    "        key = str(reason)\n",
    "        self.errors[key] = self.errors.get(key, 0) + 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def merge(self, results):\n",
    "        self.start_time = (min(self.start_time, results.start_time)\n",
    "                           if self.start_time and results.start_time\n",
    "                           else (self.start_time or results.start_time))\n",
    "        self.stop_time = (max(self.stop_time, results.stop_time)\n",
    "                              if self.stop_time and results.stop_time\n",
    "                          else (self.stop_time or results.stop_time))\n",
    "        self.msgs_ok += results.msgs_ok\n",
    "        self.msgs_fail += results.msgs_fail\n",
    "        self.latency.merge(results.latency)\n",
    "        for err in results.errors:\n",
    "            self.errors[err] = self.errors.get(err, 0) + results.errors[err]\n",
    "\n",
    "    def print_results(self):\n",
    "        if self.msgs_fail:\n",
    "            print(\"Error: %d message transfers failed\"\n",
    "                  % self.msgs_fail)\n",
    "        if self.errors:\n",
    "            print(\"Error: errors detected:\")\n",
    "            for err in self.errors:\n",
    "                print(\"  '%s' (occurred %d times)\" % (err, self.errors[err]))\n",
    "\n",
    "        total = self.msgs_ok + self.msgs_fail\n",
    "        print(\"Total Messages: %d\" % total)\n",
    "\n",
    "        delta_time = self.stop_time - self.start_time\n",
    "        print(\"Test Interval: %f - %f (%f secs)\" % (self.start_time,\n",
    "                                                    self.stop_time,\n",
    "                                                    delta_time))\n",
    "\n",
    "        if delta_time > 0.0:\n",
    "            print(\"Aggregate throughput: %f msgs/sec\" % (float(total)/delta_time))\n",
    "\n",
    "        latency = self.latency\n",
    "        if latency.count:\n",
    "            print(\"Latency %d samples (msecs): Average %f StdDev %f\"\n",
    "                  \" Min %f Max %f\"\n",
    "                  % (latency.count,\n",
    "                     latency.average(), latency.std_deviation(),\n",
    "                     latency.min, latency.max))\n",
    "            print(\"Latency Distribution: \")\n",
    "            latency.print_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util functions\n",
    "def load_stats(param):\n",
    "    \"\"\"Loads the stats for the controller output file.\"\"\"\n",
    "    try:\n",
    "        controller_docker = os.path.join(RESULT_PATH, param[\"backup_dir\"], \"*controller*.log\")\n",
    "        # beware of the files _docker.log that would also match\n",
    "        # and contains the global stats in a human readable format.\n",
    "        files = glob.glob(controller_docker)\n",
    "        controller_log = files[0]\n",
    "        if \"docker\" in controller_log:\n",
    "            controller_log = files[1]     \n",
    "        a = []\n",
    "        with open(controller_log) as f:\n",
    "            a = f.readlines()\n",
    "            stats = json.loads(a[0]), json.loads(a[1])\n",
    "            return stats\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def build_agg_results(results):\n",
    "    agg = TestResults()\n",
    "    for result in results:\n",
    "        result[\"latency\"] = Stats(**result[\"latency\"])\n",
    "        agg.merge(TestResults(**result))\n",
    "        \n",
    "    duration = agg.stop_time - agg.start_time\n",
    "    total = agg.msgs_ok + agg.msgs_fail\n",
    "    rate = float(total)/duration\n",
    "    result = agg.to_dict()\n",
    "    result[\"rate\"] = rate\n",
    "    return result\n",
    "\n",
    "def build_msgs_stats(results, msg_type):\n",
    "    # NOTE(msimonin): we don't expect a TestResult here\n",
    "    msgs = [r[msg_type] for r in results]\n",
    "    return {\n",
    "        \"mean\": statistics.mean(msgs),\n",
    "        #\"stdev\": statistics.stdev(msgs),\n",
    "        \"min\": min(msgs),\n",
    "        \"max\": max(msgs)\n",
    "    }\n",
    "\n",
    "def augment(mydict, myparams, in_key, out_key=None):\n",
    "    out_key = out_key or in_key\n",
    "    mydict.update({out_key: [p[in_key] for p in myparams]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the params from the params file\n",
    "params = []\n",
    "with open(os.path.join(RESULT_PATH, \"./params.json\")) as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wich parameters to deal with\n",
    "# this allows to test for a subset only\n",
    "PARAMS = params[:]\n",
    "\n",
    "for param in PARAMS:\n",
    "    stats = load_stats(param)\n",
    "    if not stats:\n",
    "        continue\n",
    "    clients, servers = stats\n",
    "    # what has been seen by ombt\n",
    "    param[\"_ombt_clients\"] = len(clients.values())\n",
    "    param[\"_ombt_servers\"] = len(servers.values())\n",
    "    param[\"_ombt_msgs_sent_ok\"] = build_msgs_stats(clients.values(), \"msgs_ok\")\n",
    "    param[\"_ombt_msgs_received_ok\"] = build_msgs_stats(servers.values(), \"msgs_ok\")\n",
    "    param[\"_ombt_msgs_sent_fail\"] = build_msgs_stats(clients.values(), \"msgs_fail\")\n",
    "    param[\"_ombt_msgs_received_fail\"] = build_msgs_stats(servers.values(), \"msgs_fail\")\n",
    "    #param[\"_raw_servers_test_result\"] = servers\n",
    "    #param[\"_raw_clients_test_result\"] = clients\n",
    "    param[\"_agg_servers\"] = build_agg_results(servers.values())\n",
    "    param[\"_agg_clients\"] = build_agg_results(clients.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params_calculated.json\", \"w\") as f:\n",
    "    json.dump(PARAMS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = {}\n",
    "to_extract = [\"_ombt_clients\", \"_ombt_servers\", \"executor\", \"call_type\", \"pause\", \"version\"]\n",
    "for e in to_extract:\n",
    "    augment(extraction, PARAMS, e)\n",
    "\n",
    "# Rate server side\n",
    "extraction.update({\n",
    "    \"server_rate\": [p[\"_agg_servers\"][\"rate\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Number of message processed correctly by all the servers\n",
    "extraction.update({\n",
    "    \"server_ok\": [p[\"_agg_servers\"][\"msgs_ok\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Number of message processed with a failure by all the servers\n",
    "extraction.update({\n",
    "    \"server_fail\": [p[\"_agg_servers\"][\"msgs_fail\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Average latency server side\n",
    "extraction.update({\n",
    "    \"server_latency\": [p[\"_agg_servers\"][\"latency\"][\"total\"]/p[\"_agg_servers\"][\"latency\"][\"count\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Average latency client side\n",
    "extraction.update({\n",
    "    \"client_latency\": [p[\"_agg_clients\"][\"latency\"][\"total\"]/p[\"_agg_servers\"][\"latency\"][\"count\"]for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Rate server side\n",
    "extraction.update({\n",
    "    \"client_rate\": [p[\"_agg_clients\"][\"rate\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Number of message processed correctly by all the clients\n",
    "extraction.update({\n",
    "    \"client_ok\": [p[\"_agg_clients\"][\"msgs_ok\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Number of message processed with a failure by all the servers\n",
    "extraction.update({\n",
    "    \"client_fail\": [p[\"_agg_clients\"][\"msgs_fail\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# Get a sense of what is happening on each client/server\n",
    "# min, max, avg of the number of message processed correctly by the clients\n",
    "extraction.update({\n",
    "    \"per_client_ok\": [p[\"_ombt_msgs_sent_ok\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# min, max, avg of the number of message processed with a failure by the clients\n",
    "extraction.update({\n",
    "    \"per_client_fail\": [p[\"_ombt_msgs_sent_fail\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# min, max, avg of the number of message processed correctly the servers\n",
    "extraction.update({\n",
    "    \"per_server_ok\": [p[\"_ombt_msgs_received_ok\"] for p in PARAMS]\n",
    "})\n",
    "\n",
    "# min, max, avg of the number of message processed with a failure by the servers\n",
    "extraction.update({\n",
    "    \"per_server_fail\": [p[\"_ombt_msgs_received_fail\"] for p in PARAMS]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ombt statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(extraction)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "def plot_distribution(params, client_server, index):\n",
    "    if client_server == \"client\":\n",
    "        agent = \"_agg_clients\"\n",
    "    else:\n",
    "        agent = \"_agg_servers\"        \n",
    "    distribution = params[index][agent][\"latency\"][\"distribution\"]\n",
    "    x = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    max_pw = 0\n",
    "    for p, numbers in distribution.items():\n",
    "        pw = int(p)\n",
    "        x.extend([math.log(x * 10 ** pw, 10)  for x in range(1, 11)])\n",
    "        labels.extend([10 ** pw] + 9 * [\"\"])\n",
    "        data.extend(numbers)\n",
    "        max_pw = max(pw, max_pw)\n",
    "\n",
    "    pyplot.bar(x, data, tick_label=labels, align='edge', edgecolor='black', width=-0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering metrics from influxdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "from influxdb import InfluxDBClient\n",
    "\n",
    "client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tarfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "RABBITMQ_OVERVIEW = [\n",
    "        \"messages_delivered\",\n",
    "#        \"messages_ready\",\n",
    "#        \"messages_unacked\",\n",
    "#        \"messages_acked\",\n",
    "#        \"messages_published\",\n",
    "        \"queues\",\n",
    "        \"connections\",\n",
    "        \"consumers\",\n",
    "        \"exchanges\"\n",
    "    ]\n",
    "RABBITMQ_NODE = [\n",
    "    \"mem_used\",\n",
    "    \"fd_used\",\n",
    "    \"sockets_used\"\n",
    "]\n",
    "\n",
    "for param in PARAMS:\n",
    "    # get experimentation boundaries\n",
    "    start_time = max(param['_agg_clients']['start_time'], param['_agg_servers']['start_time'])\n",
    "    stop_time = max(param['_agg_clients']['stop_time'], param['_agg_servers']['stop_time'])\n",
    "    duration = stop_time - start_time\n",
    "    start_utc = datetime.utcfromtimestamp(start_time)\n",
    "    stop_utc = datetime.utcfromtimestamp(stop_time)\n",
    "    print(\"start=%s, stop=%s\" % (start_utc, stop_utc))\n",
    "    tar = os.path.join(RESULT_PATH, param['backup_dir'], 'influxdb-data.tar.gz')\n",
    "    tarfile.open(tar).extractall()\n",
    "    # docker run --name influxdb -v $(pwd)/influxdb-data:/var/lib/influxdb -p 8083:8083 -p 8086:8086 -ti influxdb\n",
    "    # Evaluate the \"load\" of ombt-server/bus :\n",
    "    # we take the min of the usage_idle of all host in the groups ombt-server/bus\n",
    "    QUERIES = []\n",
    "    for role in ['ombt-server', 'bus']:\n",
    "        key = \"min_usage_idle_%s\" % role.replace(\"-\", \"_\")\n",
    "        query = \"SELECT min(usage_idle) as %s FROM (SELECT mean(usage_idle) as usage_idle from cpu WHERE role='%s' and time>='%s' AND time<='%s' GROUP BY host)\" % (key, role, start_utc, stop_utc)\n",
    "        QUERIES.append([\n",
    "            \"cpu\", (key), query\n",
    "        ])\n",
    "\n",
    "    # Evaluate some rabbitmq metrics\n",
    "    # Take the max during of the metrics over the interval of the experiment\n",
    "    # NOTE(msimonin): we could first group the metrics in a 10s interval and then take the max\n",
    "    for field in RABBITMQ_OVERVIEW:\n",
    "        QUERIES.append([\n",
    "            \"rabbitmq_overview\",\n",
    "            field,\n",
    "            \"SELECT max(%s) as %s from rabbitmq_overview\" % (field, field)\n",
    "        ])\n",
    "    for field in RABBITMQ_NODE:\n",
    "        QUERIES.append([\n",
    "            \"rabbitmq_node\",\n",
    "            field,\n",
    "            \"SELECT max(%s) as %s from rabbitmq_node\" % (field, field)\n",
    "        ])\n",
    "\n",
    "    print(QUERIES)\n",
    "\n",
    "    try:\n",
    "        container = client.containers.run(\n",
    "            'influxdb:latest',\n",
    "            detach=True,\n",
    "            ports={'8086/tcp': 8086, '8083/tcp': 8083},\n",
    "            volumes={os.path.join(os.getcwd(), 'influxdb-data'): {'bind': '/var/lib/influxdb', 'mode': 'rw'}}\n",
    "        )    \n",
    "        influx = InfluxDBClient(database='telegraf', timeout=600)\n",
    "        # TODO(msimonin): make a tcp socket retry test on port 8083\n",
    "        time.sleep(15)\n",
    "        \n",
    "        for serie, key,  query in QUERIES:\n",
    "            result = influx.query(query)\n",
    "            print(result)\n",
    "            result = list(result.get_points())[0]\n",
    "            key_param = \"_%s\" % serie\n",
    "            param.setdefault(key_param, {})\n",
    "            param[key_param].setdefault(key, result[key])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        container.remove(force=True)\n",
    "        subprocess.check_call(\"sudo rm -rf influxdb-data\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all rabbitmq stuffs\n",
    "for serie, field,_ in QUERIES:\n",
    "    extraction.update({\n",
    "        field : [p[\"_%s\" % serie][field] for p in PARAMS]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influxdb Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(extraction)\n",
    "pandas.options.display.max_columns = 1000\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_stats.json\", \"w\") as f:\n",
    "    f.write(df.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_stats.json\", \"r\") as f:\n",
    "    all_stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = pandas.read_json(\"all_stats.json\")\n",
    "all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = [250, 500, 750, 1000]\n",
    "ax = None\n",
    "for server in servers:\n",
    "    extract = df[(df.call_type == \"rpc-call\") & (df[\"_ombt_servers\"] == server)].loc[:, [\"_ombt_clients\", \"client_rate\"]]\n",
    "    kwargs = {\"x\": \"_ombt_clients\", \"y\": \"client_rate\"}\n",
    "    if ax:         \n",
    "        kwargs.update({\"ax\": ax})\n",
    "    ax = extract.plot(**kwargs)\n",
    "ax.legend([str(server) for server in servers])\n",
    "ax.set_title(\"rpc call message rate (msg/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = [250, 500, 750, 1000]\n",
    "ax = None\n",
    "for server in servers:\n",
    "    extract = df[(df.call_type == \"rpc-cast\") & (df[\"_ombt_servers\"] == server)].loc[:, [\"_ombt_clients\", \"server_rate\"]]\n",
    "    kwargs = {\"x\": \"_ombt_clients\", \"y\": \"server_rate\"}\n",
    "    if ax:         \n",
    "        kwargs.update({\"ax\": ax})\n",
    "    ax = extract.plot(**kwargs)\n",
    "ax.legend([str(server) for server in servers])\n",
    "ax.set_title(\"rpc cast message rate (msg/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = [250, 500, 750, 1000]\n",
    "ax = None\n",
    "for server in servers:\n",
    "    extract = df[(df.call_type == \"rpc-call\") & (df[\"_ombt_servers\"] == server)].loc[:, [\"_ombt_clients\", \"client_latency\"]]\n",
    "    kwargs = {\"x\": \"_ombt_clients\", \"y\": \"client_latency\"}\n",
    "    if ax:         \n",
    "        kwargs.update({\"ax\": ax})\n",
    "    ax = extract.plot(**kwargs)\n",
    "ax.legend([str(server) for server in servers])\n",
    "ax.set_title(\"rpc call latency (ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = [250, 500, 750, 1000]\n",
    "ax = None\n",
    "for server in servers:\n",
    "    extract = df[(df.call_type == \"rpc-cast\") & (df[\"_ombt_servers\"] == server)].loc[:, [\"_ombt_clients\", \"server_latency\"]]\n",
    "    kwargs = {\"x\": \"_ombt_clients\", \"y\": \"server_latency\"}\n",
    "    if ax:         \n",
    "        kwargs.update({\"ax\": ax})\n",
    "    ax = extract.plot(**kwargs)\n",
    "ax.legend([str(server) for server in servers])\n",
    "ax.set_title(\"rpc cast latency (ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
